<!DOCTYPE html>

<html>
    <head>
        <title>CSS Box model</title>
    </head>
    <style type="text/css">
        *{
            margin:0px;
            padding: 0px;
        }
        .box{
            border: 10px black solid;
            padding:20px;
            margin: 20px;
            margin-left: 120px;
        }



    </style>
    <body>
        <div class="box">
            <h1>Title #1</h1>
            <p> 
                Natural language understanding comprises a wide range of diverse tasks such
as textual entailment, question answering, semantic similarity assessment, and
document classification. Although large unlabeled text corpora are abundant,
labeled data for learning these specific tasks is scarce, making it challenging for
discriminatively trained models to perform adequately. We demonstrate that large
gains on these tasks can be realized by generative pre-training of a language model
on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each
specific task. In contrast to previous approaches, we make use of task-aware input
transformations during fine-tuning to achieve effective transfer while requiring
minimal changes to the model architecture. We demonstrate the effectiveness of
our approach on a wide range of benchmarks for natural language understanding.
Our general task-agnostic model outperforms discriminatively trained models that
use architectures specifically crafted for each task, significantly improving upon the
state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute
improvements of 8.9% on commonsense reasoning (Stories Cloze Test), 5.7% on
question answering (RACE), and 1.5% on textual entailment (MultiNLI).
            </p>
        </div>

    </body>
</html>